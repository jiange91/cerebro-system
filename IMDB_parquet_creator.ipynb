{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "023ecc62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T07:36:22.210398Z",
     "start_time": "2021-12-07T07:36:20.680280Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a327dcd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T07:37:00.970579Z",
     "start_time": "2021-12-07T07:36:24.434893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "b'Zero Day leads you to think, even re-think why two'\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.utils.get_file(\n",
    "    fname=\"aclImdb.tar.gz\",\n",
    "    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "    extract=True,\n",
    ")\n",
    "\n",
    "# set path to dataset\n",
    "IMDB_DATADIR = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n",
    "\n",
    "classes = [\"pos\", \"neg\"]\n",
    "train_data = load_files(\n",
    "    os.path.join(IMDB_DATADIR, \"train\"), shuffle=True, categories=classes\n",
    ")\n",
    "test_data = load_files(\n",
    "    os.path.join(IMDB_DATADIR, \"test\"), shuffle=False, categories=classes\n",
    ")\n",
    "\n",
    "x_train = np.array(train_data.data)\n",
    "y_train = np.array(train_data.target)\n",
    "x_test = np.array(test_data.data)\n",
    "y_test = np.array(test_data.target)\n",
    "\n",
    "print(x_train.shape)  # (25000,)\n",
    "print(y_train.shape)  # (25000, 1)\n",
    "print(x_train[0][:50])  # this film was just brilliant casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11835075",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T07:37:03.325969Z",
     "start_time": "2021-12-07T07:37:00.971706Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 03:31:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-07 03:31:44, Running 1 Workers\n"
     ]
    }
   ],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "\n",
    "# datas storage for intermediate data and model artifacts.\n",
    "from cerebro.storage import LocalStore, HDFSStore\n",
    "\n",
    "# Model selection/AutoML methods.\n",
    "from cerebro.tune import GridSearch, RandomSearch, TPESearch\n",
    "\n",
    "# Utility functions for specifying the search space.\n",
    "from cerebro.tune import hp_choice, hp_uniform, hp_quniform, hp_loguniform, hp_qloguniform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = '/usr/bin/python3.6'\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = '/usr/bin/python3.6'\n",
    "\n",
    "from pyspark import SparkConf\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Cerebro Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "...\n",
    "work_dir = '/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/'\n",
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=1)\n",
    "store = LocalStore(prefix_path=work_dir + 'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9fe990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T07:37:16.380874Z",
     "start_time": "2021-12-07T07:37:15.938443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.stack([x_train, y_train], axis=1)\n",
    "test_data = np.stack([x_test, y_test], axis=1)\n",
    "all_data = np.concatenate([train_data, test_data], axis=0)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1564747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T07:37:19.021361Z",
     "start_time": "2021-12-07T07:37:19.018748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9133f9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T07:37:27.268521Z",
     "start_time": "2021-12-07T07:37:21.798201Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 01:36:05 WARN TaskSetManager: Stage 0 contains a task of very large size (5408 KB). The maximum recommended task size is 100 KB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+\n",
      "|            features|label|    label_OHE|\n",
      "+--------------------+-----+-------------+\n",
      "|Zero Day leads yo...|    1|(2,[1],[1.0])|\n",
      "|Words can't descr...|    0|(2,[0],[1.0])|\n",
      "|Everyone plays th...|    1|(2,[1],[1.0])|\n",
      "|There are a lot o...|    0|(2,[0],[1.0])|\n",
      "|I've just had the...|    0|(2,[0],[1.0])|\n",
      "+--------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 01:36:06 WARN TaskSetManager: Stage 2 contains a task of very large size (5408 KB). The maximum recommended task size is 100 KB.\n"
     ]
    }
   ],
   "source": [
    "dff = map(lambda x: (x[0].decode('UTF-8') ,int(x[1])), all_data)\n",
    "mydf = spark.createDataFrame(dff,schema=[\"features\", \"label\"])\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "encoder = OneHotEncoderEstimator(dropLast=False)\n",
    "encoder.setInputCols([\"label\"])\n",
    "encoder.setOutputCols([\"label_OHE\"])\n",
    "\n",
    "encoder_model = encoder.fit(mydf)\n",
    "encoded = encoder_model.transform(mydf)\n",
    "\n",
    "feature_columns=['features']\n",
    "label_columns=['label_OHE']\n",
    "\n",
    "encoded.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764bbaa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T07:39:32.332365Z",
     "start_time": "2021-12-07T07:39:30.839253Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 01:36:10 WARN TaskSetManager: Stage 3 contains a task of very large size (5408 KB). The maximum recommended task size is 100 KB.\n",
      "[Stage 3:>                                                        (0 + 12) / 12]21/12/07 01:36:11 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "21/12/07 01:36:11 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "21/12/07 01:36:11 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "21/12/07 01:36:11 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "21/12/07 01:36:11 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "21/12/07 01:36:11 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 56.31% for 12 writers\n",
      "21/12/07 01:36:11 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "21/12/07 01:36:11 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "21/12/07 01:36:11 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "21/12/07 01:36:11 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "21/12/07 01:36:11 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "encoded.write.save(\"/Users/zijian/Desktop/ucsd/cse234/project/imdb/imdb.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad36774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|            features|    label_OHE|\n",
      "+--------------------+-------------+\n",
      "|Lauren Himmel's d...|(2,[1],[1.0])|\n",
      "|The Pickle was th...|(2,[1],[1.0])|\n",
      "|This is a great o...|(2,[1],[1.0])|\n",
      "|I liked this show...|(2,[1],[1.0])|\n",
      "|I have watched Fa...|(2,[1],[1.0])|\n",
      "+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_columns=['features']\n",
    "label_columns=['label_OHE']\n",
    "df = spark.read.load(\"/Users/zijian/Desktop/ucsd/cse234/project/imdb/imdb.parquet\")\n",
    "df = df.select(feature_columns+label_columns)\n",
    "df.show(5)\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084c3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner.engine import hyperparameters\n",
    "import autokeras as ak\n",
    "from cerebro.nas.hphpmodel import HyperHyperModel\n",
    "\n",
    "input_node = ak.TextInput()\n",
    "output_node = ak.TextBlock(block_type=\"ngram\")(input_node)\n",
    "output_node = ak.ClassificationHead(num_classes=2, multi_label=True)(output_node)\n",
    "am = HyperHyperModel(input_node, output_node, seed=2000)\n",
    "\n",
    "am.resource_bind(\n",
    "    backend=backend, \n",
    "    store=store,\n",
    "    feature_columns=feature_columns,\n",
    "    label_columns=label_columns,\n",
    "    evaluation_metric='accuracy', \n",
    ")\n",
    "\n",
    "am.tuner_bind(\n",
    "    tuner=\"greedy\", \n",
    "#     tuner=\"randomsearch\",\n",
    "    hyperparameters=None, \n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=2,\n",
    "    overwrite=True,\n",
    "    exploration=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a1a9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-07 03:31:47, Preparing Data\n",
      "CEREBRO => Time: 2021-12-07 03:31:47, Num Partitions: 12\n",
      "CEREBRO => Time: 2021-12-07 03:31:47, Writing DataFrames\n",
      "CEREBRO => Time: 2021-12-07 03:31:47, Train Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_train_data\n",
      "CEREBRO => Time: 2021-12-07 03:31:47, Val Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_val_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-07 03:31:50, Train Partitions: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-07 03:32:00, Val Partitions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-07 03:32:12, Train Rows: 32056\n",
      "CEREBRO => Time: 2021-12-07 03:32:12, Val Rows: 7914\n",
      "CEREBRO => Time: 2021-12-07 03:32:12, Initializing Workers\n",
      "CEREBRO => Time: 2021-12-07 03:32:12, Initializing Data Loaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 03:32:22.426482: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-12-07 03:32:22.426697: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "text_block_1/ma...|5000              |?                 \n",
      "text_block_1/te...|2                 |?                 \n",
      "text_block_1/de...|True              |?                 \n",
      "text_block_1/de...|1                 |?                 \n",
      "text_block_1/de...|256               |?                 \n",
      "text_block_1/de...|0.25              |?                 \n",
      "text_block_1/de...|256               |?                 \n",
      "classification_...|0                 |?                 \n",
      "optimizer         |adam              |?                 \n",
      "learning_rate     |0.001             |?                 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 03:32:22.925167: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Save or restore weights that is not an instance of `tf.Variable` is not supported in h5, use `save_format='tf'` instead. Got a model or layer TextVectorization with weights [<tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x180882a50>, <tf.Variable 'idf:0' shape=(5000,) dtype=float32, numpy=\narray([2.3978953, 0.6931472, 0.6466272, ..., 2.3978953, 2.3978953,\n       2.3978953], dtype=float32)>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dg/4jghvsmd1dl77gj1fqmzzf_80000gn/T/ipykernel_22452/3031083850.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/ucsd/cse234/project/cerebro-system/cerebro/nas/hphpmodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df, batch_size, epochs, callbacks, verbose, input_shape, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mdataset_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         )\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ucsd/cse234/project/cerebro-system/cerebro/nas/tuners/greedy.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, epochs, callbacks, validation_split, verbose, dataset_idx, metadata, cold_start, **fit_kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_trials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ucsd/cse234/project/cerebro-system/cerebro/nas/tuners/greedy.py\u001b[0m in \u001b[0;36mrun_trials\u001b[0;34m(self, trials, epochs, dataset_idx, metadata, **fit_kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mtrain_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_for_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mupdate_model_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mupdate_model_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_results_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ucsd/cse234/project/cerebro-system/cerebro/backend/spark/backend.py\u001b[0m in \u001b[0;36mtrain_for_one_epoch\u001b[0;34m(self, models, store, dataset_idx, feature_col, label_col, is_train)\u001b[0m\n\u001b[1;32m    180\u001b[0m         sub_epoch_trainers = [_get_remote_trainer(model, self, store, dataset_idx, feature_col, label_col,\n\u001b[1;32m    181\u001b[0m                                                   self.settings.verbose) \\\n\u001b[0;32m--> 182\u001b[0;31m                               for model in models]\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mmodel_worker_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ucsd/cse234/project/cerebro-system/cerebro/backend/spark/backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    180\u001b[0m         sub_epoch_trainers = [_get_remote_trainer(model, self, store, dataset_idx, feature_col, label_col,\n\u001b[1;32m    181\u001b[0m                                                   self.settings.verbose) \\\n\u001b[0;32m--> 182\u001b[0;31m                               for model in models]\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mmodel_worker_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ucsd/cse234/project/cerebro-system/cerebro/backend/spark/backend.py\u001b[0m in \u001b[0;36m_get_remote_trainer\u001b[0;34m(estimator, backend, store, dataset_idx, feature_columns, label_columns, verbose)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mremote_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_output_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrun_output_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mckpt_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_output_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mremote_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_output_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   2000\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 2002\u001b[0;31m                     signatures, options, save_traces)\n\u001b[0m\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m   def save_weights(self,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    152\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    153\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 154\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    155\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;31m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_legacy_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m     \u001b[0mweight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0mweight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36m_legacy_weights\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;34m'Save or restore weights that is not an instance of `tf.Variable` is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;34m'not supported in h5, use `save_format=\\'tf\\'` instead. Got a model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m         'or layer {} with weights {}'.format(layer.__class__.__name__, weights))\n\u001b[0m\u001b[1;32m    887\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Save or restore weights that is not an instance of `tf.Variable` is not supported in h5, use `save_format='tf'` instead. Got a model or layer TextVectorization with weights [<tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x180882a50>, <tf.Variable 'idf:0' shape=(5000,) dtype=float32, numpy=\narray([2.3978953, 0.6931472, 0.6466272, ..., 2.3978953, 2.3978953,\n       2.3978953], dtype=float32)>]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "rel = am.fit(train_df,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5da21f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cerebro.tune.ModelSelectionResult at 0x1812ad810>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c855e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from cerebro.backend.spark.util import _get_metadata\n",
    "\n",
    "unischema_fields = []\n",
    "metadata = _get_metadata(train_df)\n",
    "for k in metadata.keys():\n",
    "    type = spark_to_petastorm_type(metadata[k]['spark_data_type'])\n",
    "    shape = petastorm_unischema_shape(metadata[k]['shape'])\n",
    "    codec = petastorm_unischema_codec(metadata[k]['shape'], metadata[k]['spark_data_type'])\n",
    "    unischema_fields.append(UnischemaField(k, type, shape, codec, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccb90f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UnischemaField(name='features', numpy_dtype=<class 'numpy.uint8'>, shape=(), codec=<petastorm.codecs.ScalarCodec object at 0x17b1d1190>, nullable=False),\n",
       " UnischemaField(name='label', numpy_dtype=<class 'numpy.int64'>, shape=(), codec=<petastorm.codecs.ScalarCodec object at 0x17b1d11d0>, nullable=False),\n",
       " UnischemaField(name='label_OHE', numpy_dtype=<class 'numpy.float64'>, shape=(2,), codec=<petastorm.codecs.NdarrayCodec object at 0x17b1d10d0>, nullable=False)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unischema_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a67c7a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': {'spark_data_type': pyspark.sql.types.StringType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': 1,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': 1},\n",
       " 'label': {'spark_data_type': pyspark.sql.types.LongType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': 1,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': 1},\n",
       " 'label_OHE': {'spark_data_type': pyspark.ml.linalg.SparseVector,\n",
       "  'is_sparse_vector_only': True,\n",
       "  'shape': 2,\n",
       "  'intermediate_format': 'custom_sparse_format',\n",
       "  'max_size': 1}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1691862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unischema(petastorm_schema, [\n",
      "  UnischemaField('features', uint8, (), <petastorm.codecs.ScalarCodec object at 0x17b1d1190>, False),\n",
      "  UnischemaField('label', int64, (), <petastorm.codecs.ScalarCodec object at 0x17b1d11d0>, False),\n",
      "  UnischemaField('label_OHE', float64, (2,), <petastorm.codecs.NdarrayCodec object at 0x17b1d10d0>, False),\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "from petastorm.unischema import Unischema, UnischemaField, dict_to_spark_row\n",
    "petastorm_schema = Unischema('petastorm_schema', unischema_fields)\n",
    "print(petastorm_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0092e130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'features': '\"... the beat is too strong ... we\\'re deaf mutants now--like them\", Rex Voorhas Ormine<br /><br />I am surprised that this movie has been uniformly bashed. Let me be the first to actually discuss the virtues of \"The Beat\" and why YOU MUST SEE THIS FILM NOW.<br /><br />Make no mistake, this movie is cheesy and \"bad\" in the conventional sense: the story is preposterous, the poetry is silly, and the acting is inconsistent.<br /><br />But these are the film\\'s CHARMS--all of these ingredients form the recipe for one of the most UNDERAPPRECIATED CHEEZY FILMS of the 80\\'s.<br /><br />If the reference to \"deaf mutants\" didn\\'t pique your interest, then perhaps this will: What kind of name is \"Rex Voorhas Ormine\", anyway? It is such an unusual name (for North American audiences) that I said to myself, \"even the names of the characters in this friggin\\' movie are firggin\\' silly.\"<br /><br />Well, \"The Beat\" is so fabulously cheezy that the \"meaning\" and \"symbolism\" behind \"Rex Voorhas Ormine\" is revealed not-too-subtly by Bart Waxman (the misguided guidance counselor you love to hate). I won\\'t spoil the revelation behind Rex\\'s name, but please don\\'t get too excited, O.K.?<br /><br />Overall, the acting is inconsistent (John Savage--who plays the \"concerned teacher\" Mr. Ellsworth is pretty good, as is the fellow playing Bart Waxman, but the rest of the cast are unconvincing). That said, the acting does NOT detract from the film. Why? There is a SINCERITY in each of the actors\\' performances that makes the characters they play endearing. So although the performances may suck, you are still left with the impression that the actors are really trying to do their best. As a result, the actors\\' sincerity succeeds where their acting fails (which is quite often).<br /><br />The homage to \"beat poetry\" in this film is bad, bad, bad. But this is a good, good, good thing when it comes to entertainment. Would you actually enjoy \"better quality\" or \"more respectable\" poetry--especially in a film like this?<br /><br />Folks, that would be BORING (think about the droll they made us read in high school--sanitized to avoid \"corrupting the youth\", politically conservative and devoid of any critical analysis, etc.) Even if you don\\'t like poetry or \"arty\" movies (with all of the \"intellectual\" posturing that implies), you most certainly can (and should) appreciate LUDICROUS POETRY in a WANNABE ART FILM!!!! How could you not enjoy the following?<br /><br />\"do you remember the roar of the dinosaur? a woman\\'s scotty craps on the floor bad scotty bad, oh the woman\\'s so sad she washes her hands and then waits by the door today, yeah--today!\"<br /><br />Yes, that is an example of some of the remarkable poetry liberally sprinkled throughout \"The Beat.\" But what about the story, you ask?<br /><br />Well, the story is preposterous. But then again, that is the beauty of this film. Apart from some cliches, stereotypes, and predictable plot points, there are enough genuinely unique elements to the plot/story to keep things interesting. Who is Rex? Where did he come from? What the heck is he talking about? Deaf mutants? Illiterate angels? Do Billy and Kate REALLY understand what Rex is saying? Is the audience supposed to understand Rex and his poetry posse? (I\\'ve seen the movie several times and I still haven\\'t figured everything out.)<br /><br />Will bad poetry and high school talent shows really END GANG VIOLENCE?<br /><br />I guarantee that you have never seen anything quite like \"The Beat\"--a perfect combination of brilliantly bad poetry, mediocre-yet-sincere acting, and a \"mythopoetics conquers gang violence\" storyline that has YET TO BE RIVALLED BY ANY FILM EVER MADE.<br /><br />Bonus for fans of classic NYC hardcore: The Cro-Mags make a rare film appearance as the \"Iron Skulls\" and it\\'s a hoot to see them perform several songs. I wish they included more concert footage, but maybe that will be an \"extra\" included on the \"collector\\'s edition\" DVD I fantasize about.<br /><br />', 'label': 1, 'label_OHE': SparseVector(2, {1: 1.0})}]\n"
     ]
    }
   ],
   "source": [
    "print(train_df.rdd \\\n",
    "  .map(lambda x: x.asDict()) \\\n",
    "  .take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a803f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'features': array([34, 46, 46, ..., 32, 47, 62], dtype=uint8), 'label': array(1), 'label_OHE': array([0., 1.])}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(train_df.rdd \\\n",
    "      .map(lambda x: x.asDict()) \\\n",
    "      .map(lambda x: {k: create_array(x,k,spark_to_petastorm_type(metadata[k]['spark_data_type'])) for k in x}) \\\n",
    "      .take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84e1ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_np(x,k,dtype):\n",
    "    if dtype == np.uint8:\n",
    "        return x[k]\n",
    "    else:\n",
    "        return np.array(x[k], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1062862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'features': '\"... the beat is too strong ... we\\'re deaf mutants now--like them\", Rex Voorhas Ormine<br /><br />I am surprised that this movie has been uniformly bashed. Let me be the first to actually discuss the virtues of \"The Beat\" and why YOU MUST SEE THIS FILM NOW.<br /><br />Make no mistake, this movie is cheesy and \"bad\" in the conventional sense: the story is preposterous, the poetry is silly, and the acting is inconsistent.<br /><br />But these are the film\\'s CHARMS--all of these ingredients form the recipe for one of the most UNDERAPPRECIATED CHEEZY FILMS of the 80\\'s.<br /><br />If the reference to \"deaf mutants\" didn\\'t pique your interest, then perhaps this will: What kind of name is \"Rex Voorhas Ormine\", anyway? It is such an unusual name (for North American audiences) that I said to myself, \"even the names of the characters in this friggin\\' movie are firggin\\' silly.\"<br /><br />Well, \"The Beat\" is so fabulously cheezy that the \"meaning\" and \"symbolism\" behind \"Rex Voorhas Ormine\" is revealed not-too-subtly by Bart Waxman (the misguided guidance counselor you love to hate). I won\\'t spoil the revelation behind Rex\\'s name, but please don\\'t get too excited, O.K.?<br /><br />Overall, the acting is inconsistent (John Savage--who plays the \"concerned teacher\" Mr. Ellsworth is pretty good, as is the fellow playing Bart Waxman, but the rest of the cast are unconvincing). That said, the acting does NOT detract from the film. Why? There is a SINCERITY in each of the actors\\' performances that makes the characters they play endearing. So although the performances may suck, you are still left with the impression that the actors are really trying to do their best. As a result, the actors\\' sincerity succeeds where their acting fails (which is quite often).<br /><br />The homage to \"beat poetry\" in this film is bad, bad, bad. But this is a good, good, good thing when it comes to entertainment. Would you actually enjoy \"better quality\" or \"more respectable\" poetry--especially in a film like this?<br /><br />Folks, that would be BORING (think about the droll they made us read in high school--sanitized to avoid \"corrupting the youth\", politically conservative and devoid of any critical analysis, etc.) Even if you don\\'t like poetry or \"arty\" movies (with all of the \"intellectual\" posturing that implies), you most certainly can (and should) appreciate LUDICROUS POETRY in a WANNABE ART FILM!!!! How could you not enjoy the following?<br /><br />\"do you remember the roar of the dinosaur? a woman\\'s scotty craps on the floor bad scotty bad, oh the woman\\'s so sad she washes her hands and then waits by the door today, yeah--today!\"<br /><br />Yes, that is an example of some of the remarkable poetry liberally sprinkled throughout \"The Beat.\" But what about the story, you ask?<br /><br />Well, the story is preposterous. But then again, that is the beauty of this film. Apart from some cliches, stereotypes, and predictable plot points, there are enough genuinely unique elements to the plot/story to keep things interesting. Who is Rex? Where did he come from? What the heck is he talking about? Deaf mutants? Illiterate angels? Do Billy and Kate REALLY understand what Rex is saying? Is the audience supposed to understand Rex and his poetry posse? (I\\'ve seen the movie several times and I still haven\\'t figured everything out.)<br /><br />Will bad poetry and high school talent shows really END GANG VIOLENCE?<br /><br />I guarantee that you have never seen anything quite like \"The Beat\"--a perfect combination of brilliantly bad poetry, mediocre-yet-sincere acting, and a \"mythopoetics conquers gang violence\" storyline that has YET TO BE RIVALLED BY ANY FILM EVER MADE.<br /><br />Bonus for fans of classic NYC hardcore: The Cro-Mags make a rare film appearance as the \"Iron Skulls\" and it\\'s a hoot to see them perform several songs. I wish they included more concert footage, but maybe that will be an \"extra\" included on the \"collector\\'s edition\" DVD I fantasize about.<br /><br />', 'label': array(1), 'label_OHE': array([0., 1.])}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(train_df.rdd \\\n",
    "      .map(lambda x: x.asDict()) \\\n",
    "      .map(lambda x: {k: map_to_np(x,k,spark_to_petastorm_type(metadata[k]['spark_data_type'])) for k in x}) \\\n",
    "      .take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7eae4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(features='\"... the beat is too strong ... we\\'re deaf mutants now--like them\", Rex Voorhas Ormine<br /><br />I am surprised that this movie has been uniformly bashed. Let me be the first to actually discuss the virtues of \"The Beat\" and why YOU MUST SEE THIS FILM NOW.<br /><br />Make no mistake, this movie is cheesy and \"bad\" in the conventional sense: the story is preposterous, the poetry is silly, and the acting is inconsistent.<br /><br />But these are the film\\'s CHARMS--all of these ingredients form the recipe for one of the most UNDERAPPRECIATED CHEEZY FILMS of the 80\\'s.<br /><br />If the reference to \"deaf mutants\" didn\\'t pique your interest, then perhaps this will: What kind of name is \"Rex Voorhas Ormine\", anyway? It is such an unusual name (for North American audiences) that I said to myself, \"even the names of the characters in this friggin\\' movie are firggin\\' silly.\"<br /><br />Well, \"The Beat\" is so fabulously cheezy that the \"meaning\" and \"symbolism\" behind \"Rex Voorhas Ormine\" is revealed not-too-subtly by Bart Waxman (the misguided guidance counselor you love to hate). I won\\'t spoil the revelation behind Rex\\'s name, but please don\\'t get too excited, O.K.?<br /><br />Overall, the acting is inconsistent (John Savage--who plays the \"concerned teacher\" Mr. Ellsworth is pretty good, as is the fellow playing Bart Waxman, but the rest of the cast are unconvincing). That said, the acting does NOT detract from the film. Why? There is a SINCERITY in each of the actors\\' performances that makes the characters they play endearing. So although the performances may suck, you are still left with the impression that the actors are really trying to do their best. As a result, the actors\\' sincerity succeeds where their acting fails (which is quite often).<br /><br />The homage to \"beat poetry\" in this film is bad, bad, bad. But this is a good, good, good thing when it comes to entertainment. Would you actually enjoy \"better quality\" or \"more respectable\" poetry--especially in a film like this?<br /><br />Folks, that would be BORING (think about the droll they made us read in high school--sanitized to avoid \"corrupting the youth\", politically conservative and devoid of any critical analysis, etc.) Even if you don\\'t like poetry or \"arty\" movies (with all of the \"intellectual\" posturing that implies), you most certainly can (and should) appreciate LUDICROUS POETRY in a WANNABE ART FILM!!!! How could you not enjoy the following?<br /><br />\"do you remember the roar of the dinosaur? a woman\\'s scotty craps on the floor bad scotty bad, oh the woman\\'s so sad she washes her hands and then waits by the door today, yeah--today!\"<br /><br />Yes, that is an example of some of the remarkable poetry liberally sprinkled throughout \"The Beat.\" But what about the story, you ask?<br /><br />Well, the story is preposterous. But then again, that is the beauty of this film. Apart from some cliches, stereotypes, and predictable plot points, there are enough genuinely unique elements to the plot/story to keep things interesting. Who is Rex? Where did he come from? What the heck is he talking about? Deaf mutants? Illiterate angels? Do Billy and Kate REALLY understand what Rex is saying? Is the audience supposed to understand Rex and his poetry posse? (I\\'ve seen the movie several times and I still haven\\'t figured everything out.)<br /><br />Will bad poetry and high school talent shows really END GANG VIOLENCE?<br /><br />I guarantee that you have never seen anything quite like \"The Beat\"--a perfect combination of brilliantly bad poetry, mediocre-yet-sincere acting, and a \"mythopoetics conquers gang violence\" storyline that has YET TO BE RIVALLED BY ANY FILM EVER MADE.<br /><br />Bonus for fans of classic NYC hardcore: The Cro-Mags make a rare film appearance as the \"Iron Skulls\" and it\\'s a hoot to see them perform several songs. I wish they included more concert footage, but maybe that will be an \"extra\" included on the \"collector\\'s edition\" DVD I fantasize about.<br /><br />', label=1, label_OHE=bytearray(b\"\\x93NUMPY\\x01\\x00v\\x00{\\'descr\\': \\'<f8\\', \\'fortran_order\\': False, \\'shape\\': (2,), }                                                            \\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\"))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(train_df.rdd \\\n",
    "      .map(lambda x: x.asDict()) \\\n",
    "      .map(lambda x: {k: map_to_np(x,k,spark_to_petastorm_type(metadata[k]['spark_data_type'])) for k in x}) \\\n",
    "      .map(lambda x: dict_to_spark_row(petastorm_schema, x))\n",
    "      .take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d56043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerebro.backend.spark import util\n",
    "_, _, meta, avg_row_size = util.get_simple_meta_from_parquet(store, feature_columns+label_columns, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b080f4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': {'spark_data_type': pyspark.sql.types.StringType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'label_OHE': {'spark_data_type': pyspark.sql.types.BinaryType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf9f095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
