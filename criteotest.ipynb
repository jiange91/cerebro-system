{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912440cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:22:52.203081Z",
     "start_time": "2021-12-05T13:22:48.243701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-05 06:22:52, Running 4 Workers\n"
     ]
    }
   ],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "\n",
    "# datas storage for intermediate data and model artifacts.\n",
    "from cerebro.storage import LocalStore, HDFSStore\n",
    "\n",
    "# Model selection/AutoML methods.\n",
    "from cerebro.tune import GridSearch, RandomSearch, TPESearch\n",
    "\n",
    "# Utility functions for specifying the search space.\n",
    "from cerebro.tune import hp_choice, hp_uniform, hp_quniform, hp_loguniform, hp_qloguniform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = '/usr/bin/python3.6'\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = '/usr/bin/python3.6'\n",
    "\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('cluster') \\\n",
    "    .setMaster('spark://10.10.1.1:7077') \\\n",
    "    .set('spark.task.cpus', '16') \\\n",
    "    .set('spark.executor.memory', '124g')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.addPyFile(\"cerebro.zip\")\n",
    "\n",
    "work_dir = '/var/nfs/'\n",
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=4)\n",
    "store = LocalStore(prefix_path=work_dir + 'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d40157f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:22:55.861480Z",
     "start_time": "2021-12-05T13:22:52.204834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 0.615704% of training data, with 1624157 rows in the original data\n",
      "Use 0.554204% of testing data, with 180439 rows in the original data\n"
     ]
    }
   ],
   "source": [
    "TRAIN_NUM = 10000\n",
    "TEST_NUM = 1000\n",
    "\n",
    "# train_df = spark.read.format(\"parquet\").option('header', 'true').option('inferSchema', 'true')\\\n",
    "#     .load(work_dir+'data/parquet/train/train_0.parquet')\n",
    "# test_df = spark.read.format(\"parquet\").option('header', 'true').option('inferSchema', 'true')\\\n",
    "#     .load(work_dir+'data/parquet/valid/valid_0.parquet')\n",
    "train_df = spark.read.parquet(work_dir+'data/parquet/train/train_0.parquet')\n",
    "# train_df = spark.read.parquet('/var/nfs/tmp/data/train.parquet')\n",
    "test_df = spark.read.parquet(work_dir+'data/parquet/valid/valid_0.parquet')\n",
    "\n",
    "train_row_nums = train_df.count()\n",
    "test_row_nums = test_df.count()\n",
    "\n",
    "train_data_ratio = TRAIN_NUM / train_row_nums\n",
    "test_data_ratio = TEST_NUM / test_row_nums\n",
    "\n",
    "print(\"Use {:%} of training data, with {} rows in the original data\".format(train_data_ratio, train_row_nums))\n",
    "print(\"Use {:%} of testing data, with {} rows in the original data\".format(test_data_ratio, test_row_nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82466306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:23:37.191005Z",
     "start_time": "2021-12-05T13:23:37.181129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- labels: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- labels: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59ed897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:23:41.641598Z",
     "start_time": "2021-12-05T13:23:41.634139Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "# train_df = train_df.orderBy(rand())\n",
    "# test_df = test_df.orderBy(rand())\n",
    "train_df = train_df.limit(TRAIN_NUM)\n",
    "\n",
    "test_df = test_df.limit(TEST_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "306f356c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:23:47.317418Z",
     "start_time": "2021-12-05T13:23:47.260459Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras_tuner.engine import hyperparameters\n",
    "import autokeras as ak\n",
    "from cerebro.nas.hphpmodel import HyperHyperModel\n",
    "\n",
    "# Define the search space\n",
    "input_node = ak.StructuredDataInput()\n",
    "otuput_node = ak.DenseBlock()(input_node)\n",
    "output_node = ak.ClassificationHead()(otuput_node)\n",
    "\n",
    "am = HyperHyperModel(input_node, output_node, seed=2500)\n",
    "\n",
    "am.resource_bind(\n",
    "    backend=backend, \n",
    "    store=store,\n",
    "    feature_columns=[\"features\"],\n",
    "    label_columns=['labels'],\n",
    "    evaluation_metric='accuracy', \n",
    ")\n",
    "\n",
    "am.tuner_bind(\n",
    "    tuner=\"greedy\", \n",
    "    hyperparameters=None, \n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,\n",
    "    overwrite=True,\n",
    "    exploration=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08fc0924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:30:08.483864Z",
     "start_time": "2021-12-05T13:25:56.834547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-05 06:25:56, Num Partitions: 12\n",
      "CEREBRO => Time: 2021-12-05 06:25:56, Writing DataFrames\n",
      "CEREBRO => Time: 2021-12-05 06:25:56, Train Data Path: file:///var/nfs/test/intermediate_train_data\n",
      "CEREBRO => Time: 2021-12-05 06:25:56, Val Data Path: file:///var/nfs/test/intermediate_val_data\n",
      "CEREBRO => Time: 2021-12-05 06:28:13, Train Partitions: 9\n",
      "CEREBRO => Time: 2021-12-05 06:28:23, Val Partitions: 4\n",
      "CEREBRO => Time: 2021-12-05 06:30:08, Train Rows: 7936\n",
      "CEREBRO => Time: 2021-12-05 06:30:08, Val Rows: 2064\n"
     ]
    }
   ],
   "source": [
    "ms = am.model_selection\n",
    "_, _, metadata, _ = ms.backend.prepare_data(ms.store, train_df, ms.validation, label_columns=ms.label_cols, feature_columns=ms.feature_cols,num_partitions=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e30fa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-04T13:14:07.563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-04 06:14:15, Preparing Data\n",
      "CEREBRO => Time: 2021-12-04 06:14:15, Num Partitions: 1\n",
      "CEREBRO => Time: 2021-12-04 06:14:15, Writing DataFrames\n",
      "CEREBRO => Time: 2021-12-04 06:14:15, Train Data Path: file:///var/nfs/test/intermediate_train_data\n",
      "CEREBRO => Time: 2021-12-04 06:14:15, Val Data Path: file:///var/nfs/test/intermediate_val_data\n"
     ]
    }
   ],
   "source": [
    "rel = am.fit(train_df, epochs=10)\n",
    "\n",
    "import json\n",
    "m = {}\n",
    "for model in rel.metrics:\n",
    "    m[model] = {}\n",
    "    for key in rel.metrics[model]:\n",
    "        if key != 'trial':\n",
    "            m[model][key] = rel.metrics[model][key]\n",
    "\n",
    "with open(\"criteo_nas_dev/metrics.txt\", \"w\") as file:\n",
    "    file.write(json.dumps(m))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
