{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fbcc5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from keras_tuner import HyperParameters\n",
    "\n",
    "import autokeras as ak\n",
    "\n",
    "from cerebro.nas.hphpmodel import HyperHyperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd8efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "\n",
    "input_node = [ak.StructuredDataInput() for col in feature_columns]\n",
    "output_node = ak.StructuredDataBlock()(input_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "am = HyperHyperModel(\n",
    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e3bcfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/20 22:01:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-20 22:01:59, Running 1 Workers\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .appName(\"Iris test\") \\\n",
    "   .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.storage import LocalStore\n",
    "\n",
    "backend = SparkBackend(spark_context=sc, num_workers=1)\n",
    "store = LocalStore(prefix_path='/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments')\n",
    "\n",
    "am.resource_bind(\n",
    "    backend=backend, \n",
    "    store=store,\n",
    "    feature_columns=feature_columns,\n",
    "    label_columns=['Species']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1dbc1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(SepalLengthCm=5.1, SepalWidthCm=3.5, PetalLengthCm=1.4, PetalWidthCm=0.2, Species=0),\n",
       " Row(SepalLengthCm=4.9, SepalWidthCm=3.0, PetalLengthCm=1.4, PetalWidthCm=0.2, Species=0),\n",
       " Row(SepalLengthCm=4.7, SepalWidthCm=3.2, PetalLengthCm=1.3, PetalWidthCm=0.2, Species=0),\n",
       " Row(SepalLengthCm=4.6, SepalWidthCm=3.1, PetalLengthCm=1.5, PetalWidthCm=0.2, Species=0),\n",
       " Row(SepalLengthCm=5.0, SepalWidthCm=3.6, PetalLengthCm=1.4, PetalWidthCm=0.2, Species=0),\n",
       " Row(SepalLengthCm=5.4, SepalWidthCm=3.9, PetalLengthCm=1.7, PetalWidthCm=0.4, Species=0),\n",
       " Row(SepalLengthCm=4.6, SepalWidthCm=3.4, PetalLengthCm=1.4, PetalWidthCm=0.3, Species=0),\n",
       " Row(SepalLengthCm=5.0, SepalWidthCm=3.4, PetalLengthCm=1.5, PetalWidthCm=0.2, Species=0),\n",
       " Row(SepalLengthCm=4.4, SepalWidthCm=2.9, PetalLengthCm=1.4, PetalWidthCm=0.2, Species=0),\n",
       " Row(SepalLengthCm=4.9, SepalWidthCm=3.1, PetalLengthCm=1.5, PetalWidthCm=0.1, Species=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/Iris_clean.csv\", header=True, inferSchema=True)\n",
    "\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4394f623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./test/oracle.json\n",
      "CEREBRO => Time: 2021-11-20 22:02:02, Preparing Data\n",
      "CEREBRO => Time: 2021-11-20 22:02:02, Num Partitions: 1\n",
      "CEREBRO => Time: 2021-11-20 22:02:02, Writing DataFrames\n",
      "CEREBRO => Time: 2021-11-20 22:02:02, Train Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments/intermediate_train_data\n",
      "CEREBRO => Time: 2021-11-20 22:02:02, Val Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments/intermediate_val_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-20 22:02:03, Train Partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-20 22:02:06, Val Partitions: 1\n",
      "CEREBRO => Time: 2021-11-20 22:02:07, Train Rows: 89\n",
      "CEREBRO => Time: 2021-11-20 22:02:07, Val Rows: 31\n",
      "CEREBRO => Time: 2021-11-20 22:02:07, Initializing Workers\n",
      "CEREBRO => Time: 2021-11-20 22:02:07, Initializing Data Loaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 22:02:07.558203: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-20 22:02:07.558467: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "learning_rate     |0.1               |?                 \n",
      "batch_size        |64                |?                 \n",
      "structured_data...|True              |?                 \n",
      "structured_data...|True              |?                 \n",
      "structured_data...|2                 |?                 \n",
      "structured_data...|64                |?                 \n",
      "structured_data...|0                 |?                 \n",
      "structured_data...|512               |?                 \n",
      "classification_...|0.5               |?                 \n",
      "optimizer         |adam              |?                 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 22:02:08.180220: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "\n",
      "['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
      "[[-1, 1], [-1, 1], [-1, 1], [-1, 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 22:02:09.028642: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-20 22:02:09.509174: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-20 22:02:09.520539: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "2021-11-20 22:02:10.382623: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "Train on 3 steps\n",
      "3/3 [==============================] - 0s 2ms/step - batch: 1.0000 - size: 1.0000 - loss: 1.1300e-07 - accuracy: 0.2812\n",
      "CEREBRO => Time: 2021-11-20 22:02:11, Model: model_0_1637474528, Mode: TRAIN, Initialization Time: 1.2343289852142334, Training Time: 0.8212118148803711, Finalization Time: 0.24328017234802246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "\n",
      "['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
      "[[-1, 1], [-1, 1], [-1, 1], [-1, 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "can only concatenate str (not \"list\") to str\nTraceback (most recent call last):\n  File \"/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/cerebro/backend/spark/service_task.py\", line 196, in bg_execute\n    local_task_index=self.local_task_index)\n  File \"/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/cerebro/backend/spark/backend.py\", line 511, in train\n    result = {k: v for k, v in zip(['val_loss'] + ['val_' + name for name in metrics_names], result)}\n  File \"/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/cerebro/backend/spark/backend.py\", line 511, in <listcomp>\n    result = {k: v for k, v in zip(['val_loss'] + ['val_' + name for name in metrics_names], result)}\nTypeError: can only concatenate str (not \"list\") to str\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dg/4jghvsmd1dl77gj1fqmzzf_80000gn/T/ipykernel_35170/3900333083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"randomsearch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/ucsd/cse234/project/cerebro-system/cerebro/nas/hphpmodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df, batch_size, epochs, callbacks, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mdataset_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         )\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ucsd/cse234/project/cerebro-system/cerebro/nas/tuners/randsearch.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, epochs, callbacks, validation_split, verbose, dataset_idx, metadata, **fit_kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ucsd/cse234/project/cerebro-system/cerebro/nas/tuners/randsearch.py\u001b[0m in \u001b[0;36mrun_trials\u001b[0;34m(self, trials, epochs, dataset_idx, metadata, **fit_kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mupdate_model_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mval_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_for_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mupdate_model_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ucsd/cse234/project/cerebro-system/cerebro/backend/spark/backend.py\u001b[0m in \u001b[0;36mtrain_for_one_epoch\u001b[0;34m(self, models, store, dataset_idx, feature_col, label_col, is_train)\u001b[0m\n\u001b[1;32m    220\u001b[0m                                 \u001b[0;31m# Application Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_epoch_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                                 \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_epoch_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: can only concatenate str (not \"list\") to str\nTraceback (most recent call last):\n  File \"/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/cerebro/backend/spark/service_task.py\", line 196, in bg_execute\n    local_task_index=self.local_task_index)\n  File \"/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/cerebro/backend/spark/backend.py\", line 511, in train\n    result = {k: v for k, v in zip(['val_loss'] + ['val_' + name for name in metrics_names], result)}\n  File \"/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/cerebro/backend/spark/backend.py\", line 511, in <listcomp>\n    result = {k: v for k, v in zip(['val_loss'] + ['val_' + name for name in metrics_names], result)}\nTypeError: can only concatenate str (not \"list\") to str\n"
     ]
    }
   ],
   "source": [
    "am.tuner_bind(tuner=\"randomsearch\", hyperparameters=None)\n",
    "am.fit(train_df, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
