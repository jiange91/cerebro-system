{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e85c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T22:59:30.114105Z",
     "start_time": "2021-12-07T22:59:26.134325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-07 15:59:30, Running 6 Workers\n"
     ]
    }
   ],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "\n",
    "# datas storage for intermediate data and model artifacts.\n",
    "from cerebro.storage import LocalStore, HDFSStore\n",
    "\n",
    "# Model selection/AutoML methods.\n",
    "from cerebro.tune import GridSearch, RandomSearch, TPESearch\n",
    "\n",
    "# Utility functions for specifying the search space.\n",
    "from cerebro.tune import hp_choice, hp_uniform, hp_quniform, hp_loguniform, hp_qloguniform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = '/usr/bin/python3.6'\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = '/usr/bin/python3.6'\n",
    "\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('cluster') \\\n",
    "    .setMaster('spark://10.10.1.1:7077') \\\n",
    "    .set('spark.task.cpus', '16') \\\n",
    "    .set('spark.executor.memory', '124g')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.addPyFile(\"cerebro.zip\")\n",
    "\n",
    "work_dir = '/var/nfs/'\n",
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=6)\n",
    "store = LocalStore(prefix_path=work_dir + 'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c30b2a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:09:00.553872Z",
     "start_time": "2021-12-07T23:09:00.547531Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "from petastorm.codecs import CompressedImageCodec, \\\n",
    "        NdarrayCodec, ScalarCodec\n",
    "from petastorm.etl.dataset_metadata import materialize_dataset\n",
    "from petastorm.unischema import Unischema,\\\n",
    "        UnischemaField, dict_to_spark_row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "def cifar_to_peta():\n",
    "    MySchema = Unischema('MySchema', [\n",
    "        UnischemaField('image', np.uint8,\n",
    "                       (32,32,3), NdarrayCodec(), False),\n",
    "        UnischemaField('label', np.uint8,\n",
    "                       (), ScalarCodec(IntegerType()), False),\n",
    "    ])\n",
    "    (data, labels), _ = datasets.cifar10.load_data()\n",
    "    labels = labels.flatten().tolist()\n",
    "    num_procs = 4 # set the number of parallel processes\n",
    "    sc = spark.sparkContext\n",
    "    num_samples = len(labels)\n",
    "    output_url = 'file:///var/nfs/cifar10'\n",
    "    rowgroup_size_mb = 1024\n",
    "    def row_generator(i):\n",
    "        return {\n",
    "            'image': data[i],\n",
    "            'label': np.uint8(labels[i]),\n",
    "        }\n",
    "    # Wrap dataset materialization portion.\n",
    "    # Will take care of setting up spark environment variables as\n",
    "    # well as save petastorm specific metadata\n",
    "    with materialize_dataset(spark, output_url,\n",
    "                             MySchema, rowgroup_size_mb):\n",
    "        rows_rdd = sc.parallelize(range(num_samples)) \\\n",
    "            .map(row_generator) \\\n",
    "            .map(lambda x: dict_to_spark_row(MySchema, x))\n",
    "        spark.createDataFrame(rows_rdd, \n",
    "                              MySchema.as_spark_schema()) \\\n",
    "            .write \\\n",
    "            .mode('overwrite') \\\n",
    "            .parquet(output_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d642f5a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:09:29.740295Z",
     "start_time": "2021-12-07T23:09:01.102565Z"
    }
   },
   "outputs": [],
   "source": [
    "cifar_to_peta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f1d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
