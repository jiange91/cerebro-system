{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c5e8b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:51:29.042038Z",
     "start_time": "2021-12-07T23:51:25.124424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-07 16:51:29, Running 6 Workers\n"
     ]
    }
   ],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "\n",
    "# datas storage for intermediate data and model artifacts.\n",
    "from cerebro.storage import LocalStore, HDFSStore\n",
    "\n",
    "# Model selection/AutoML methods.\n",
    "from cerebro.tune import GridSearch, RandomSearch, TPESearch\n",
    "\n",
    "# Utility functions for specifying the search space.\n",
    "from cerebro.tune import hp_choice, hp_uniform, hp_quniform, hp_loguniform, hp_qloguniform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = '/usr/bin/python3.6'\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = '/usr/bin/python3.6'\n",
    "\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('cluster') \\\n",
    "    .setMaster('spark://10.10.1.1:7077') \\\n",
    "    .set('spark.task.cpus', '16') \\\n",
    "    .set('spark.executor.memory', '124g')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.addPyFile(\"cerebro.zip\")\n",
    "\n",
    "work_dir = '/var/nfs/'\n",
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=6)\n",
    "store = LocalStore(prefix_path=work_dir + 'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63706cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:51:31.324777Z",
     "start_time": "2021-12-07T23:51:31.316639Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "from petastorm.codecs import CompressedImageCodec, \\\n",
    "        NdarrayCodec, ScalarCodec\n",
    "from petastorm.etl.dataset_metadata import materialize_dataset\n",
    "from petastorm.unischema import Unischema,\\\n",
    "        UnischemaField, dict_to_spark_row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "def cifar_to_peta():\n",
    "    MySchema = Unischema('MySchema', [\n",
    "        UnischemaField('image', np.uint8,\n",
    "                       (32,32,3), NdarrayCodec(), False),\n",
    "        UnischemaField('label', np.float32,\n",
    "                       (10,), NdarrayCodec(), False),\n",
    "    ])\n",
    "    (data, labels), _ = datasets.cifar10.load_data()\n",
    "    labels = keras.utils.to_categorical(labels, 10)\n",
    "    num_procs = 4 # set the number of parallel processes\n",
    "    sc = spark.sparkContext\n",
    "    num_samples = len(labels)\n",
    "    output_url = 'file:///var/nfs/cifar10/petastorm'\n",
    "    rowgroup_size_mb = 1024\n",
    "    def row_generator(i):\n",
    "        return {\n",
    "            'image': data[i],\n",
    "            'label': labels[i],\n",
    "        }\n",
    "    # Wrap dataset materialization portion.\n",
    "    # Will take care of setting up spark environment variables as\n",
    "    # well as save petastorm specific metadata\n",
    "    with materialize_dataset(spark, output_url,\n",
    "                             MySchema, rowgroup_size_mb):\n",
    "        rows_rdd = sc.parallelize(range(num_samples)) \\\n",
    "            .map(row_generator) \\\n",
    "            .map(lambda x: dict_to_spark_row(MySchema, x))\n",
    "        spark.createDataFrame(rows_rdd, \n",
    "                              MySchema.as_spark_schema()) \\\n",
    "            .write \\\n",
    "            .mode('overwrite') \\\n",
    "            .parquet(output_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396aa87c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:52:05.001859Z",
     "start_time": "2021-12-07T23:51:33.245614Z"
    }
   },
   "outputs": [],
   "source": [
    "cifar_to_peta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ef4fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:52:15.994314Z",
     "start_time": "2021-12-07T23:52:15.714001Z"
    }
   },
   "outputs": [],
   "source": [
    "(data, labels), _ = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01fd1231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:52:21.949323Z",
     "start_time": "2021-12-07T23:52:21.942173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca66832d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:52:27.435336Z",
     "start_time": "2021-12-07T23:52:27.432853Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = keras.utils.to_categorical(labels, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d7984b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:52:58.040178Z",
     "start_time": "2021-12-07T23:52:58.031761Z"
    }
   },
   "outputs": [],
   "source": [
    "with open ('/var/nfs/cifar10/prep_np/prep.npy', 'wb') as f:\n",
    "    np.save(f, data[:100])\n",
    "    np.save(f, labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a5fd5bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:52:59.506921Z",
     "start_time": "2021-12-07T23:52:59.503931Z"
    }
   },
   "outputs": [],
   "source": [
    "with open ('/var/nfs/cifar10/prep_np/prep.npy', 'rb') as f:\n",
    "    x = np.load(f)\n",
    "    y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46b90bfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:53:02.790844Z",
     "start_time": "2021-12-07T23:53:02.788444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 32, 32, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "814e1086",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:53:09.891408Z",
     "start_time": "2021-12-07T23:53:09.888746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e5adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
