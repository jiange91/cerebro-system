{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912440cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T12:30:50.266902Z",
     "start_time": "2021-12-06T12:30:46.331077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-06 05:30:50, Running 6 Workers\n"
     ]
    }
   ],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "\n",
    "# datas storage for intermediate data and model artifacts.\n",
    "from cerebro.storage import LocalStore, HDFSStore\n",
    "\n",
    "# Model selection/AutoML methods.\n",
    "from cerebro.tune import GridSearch, RandomSearch, TPESearch\n",
    "\n",
    "# Utility functions for specifying the search space.\n",
    "from cerebro.tune import hp_choice, hp_uniform, hp_quniform, hp_loguniform, hp_qloguniform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = '/usr/bin/python3.6'\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = '/usr/bin/python3.6'\n",
    "\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('cluster') \\\n",
    "    .setMaster('spark://10.10.1.1:7077') \\\n",
    "    .set('spark.task.cpus', '16') \\\n",
    "    .set('spark.executor.memory', '124g')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.addPyFile(\"cerebro.zip\")\n",
    "\n",
    "work_dir = '/var/nfs/'\n",
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=6)\n",
    "store = LocalStore(prefix_path=work_dir + 'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60165d54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T12:30:50.339688Z",
     "start_time": "2021-12-06T12:30:50.268530Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras_tuner.engine import hyperparameters\n",
    "import autokeras as ak\n",
    "from cerebro.nas.hphpmodel import HyperHyperModel\n",
    "\n",
    "# Define the search space\n",
    "input_node = ak.StructuredDataInput()\n",
    "otuput_node = ak.DenseBlock()(input_node)\n",
    "output_node = ak.ClassificationHead()(otuput_node)\n",
    "\n",
    "am = HyperHyperModel(input_node, output_node, seed=2500)\n",
    "\n",
    "am.resource_bind(\n",
    "    backend=backend, \n",
    "    store=store,\n",
    "    feature_columns=[\"features\"],\n",
    "    label_columns=['labels'],\n",
    "    evaluation_metric='accuracy', \n",
    ")\n",
    "\n",
    "am.tuner_bind(\n",
    "    tuner=\"greedy\", \n",
    "    hyperparameters=None, \n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,\n",
    "    overwrite=True,\n",
    "    exploration=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0ff658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T12:32:21.147797Z",
     "start_time": "2021-12-06T12:32:16.022117Z"
    }
   },
   "outputs": [],
   "source": [
    "ms = am.model_selection\n",
    "\n",
    "_, _, metadata, _ = ms.backend.get_metadata_from_parquet(ms.store, ms.label_cols, ms.feature_cols)\n",
    "ms.backend.initialize_workers()\n",
    "ms.backend.initialize_data_loaders(ms.store, None, ms.feature_cols + ms.label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57128884",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T12:35:00.133327Z",
     "start_time": "2021-12-06T12:34:59.940575Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/petastorm/reader.py:487: UserWarning: No matching data is available for loading after rowgroup selector were applied and the data was sharded.\n",
      "  warnings.warn('No matching data is available for loading after rowgroup '\n"
     ]
    }
   ],
   "source": [
    "train_reader, val_reader = ms.backend.data_readers_fn(40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d40157f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T11:54:54.961181Z",
     "start_time": "2021-12-06T11:54:51.042573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 0.006157% of training data, with 1624157 rows in the original data\n",
      "Use 0.055420% of testing data, with 180439 rows in the original data\n"
     ]
    }
   ],
   "source": [
    "TRAIN_NUM = 100\n",
    "TEST_NUM = 100\n",
    "\n",
    "# train_df = spark.read.format(\"parquet\").option('header', 'true').option('inferSchema', 'true')\\\n",
    "#     .load(work_dir+'data/parquet/train/train_0.parquet')\n",
    "# test_df = spark.read.format(\"parquet\").option('header', 'true').option('inferSchema', 'true')\\\n",
    "#     .load(work_dir+'data/parquet/valid/valid_0.parquet')\n",
    "train_df = spark.read.parquet(work_dir+'criteo/parquet/train/train_0.parquet')\n",
    "# train_df = spark.read.parquet('/var/nfs/tmp/data/train.parquet')\n",
    "test_df = spark.read.parquet(work_dir+'criteo/parquet/valid/valid_0.parquet')\n",
    "\n",
    "train_row_nums = train_df.count()\n",
    "test_row_nums = test_df.count()\n",
    "\n",
    "train_data_ratio = TRAIN_NUM / train_row_nums\n",
    "test_data_ratio = TEST_NUM / test_row_nums\n",
    "\n",
    "print(\"Use {:%} of training data, with {} rows in the original data\".format(train_data_ratio, train_row_nums))\n",
    "print(\"Use {:%} of testing data, with {} rows in the original data\".format(test_data_ratio, test_row_nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82466306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T11:58:33.445520Z",
     "start_time": "2021-12-06T11:58:33.442769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- labels: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- labels: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e59ed897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T11:58:15.747566Z",
     "start_time": "2021-12-06T11:56:27.215929Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "# train_df = train_df.orderBy(rand())\n",
    "# test_df = test_df.orderBy(rand())\n",
    "train_df_lm = train_df.limit(TRAIN_NUM)\n",
    "\n",
    "test_df_lm = test_df.limit(TEST_NUM)\n",
    "\n",
    "train_df_lm.write.parquet(work_dir+\"limit/criteo/train.parquet\")\n",
    "test_df_lm.write.parquet(work_dir+\"limit/criteo/test.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4371daff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T12:02:26.973658Z",
     "start_time": "2021-12-06T12:02:26.749490Z"
    }
   },
   "outputs": [],
   "source": [
    "from cerebro.backend.spark.util import _get_dataset_info\n",
    "train_data_path = store.get_train_data_path(None)\n",
    "train_data = ms.store.get_parquet_dataset(train_data_path)\n",
    "schema = train_data.schema.to_arrow_schema()\n",
    "train_rows, total_byte_size = _get_dataset_info(train_data, 'training', train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39ef6ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T12:02:57.306732Z",
     "start_time": "2021-12-06T12:02:57.304219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12993256"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306f356c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T12:29:59.044414Z",
     "start_time": "2021-12-06T12:29:58.962279Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras_tuner.engine import hyperparameters\n",
    "import autokeras as ak\n",
    "from cerebro.nas.hphpmodel import HyperHyperModel\n",
    "\n",
    "# Define the search space\n",
    "input_node = ak.StructuredDataInput()\n",
    "otuput_node = ak.DenseBlock()(input_node)\n",
    "output_node = ak.ClassificationHead()(otuput_node)\n",
    "\n",
    "am = HyperHyperModel(input_node, output_node, seed=2500)\n",
    "\n",
    "am.resource_bind(\n",
    "    backend=backend, \n",
    "    store=store,\n",
    "    feature_columns=[\"features\"],\n",
    "    label_columns=['labels'],\n",
    "    evaluation_metric='accuracy', \n",
    ")\n",
    "\n",
    "am.tuner_bind(\n",
    "    tuner=\"greedy\", \n",
    "    hyperparameters=None, \n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,\n",
    "    overwrite=True,\n",
    "    exploration=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e30fa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-04T13:14:07.563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-04 06:14:15, Preparing Data\n",
      "CEREBRO => Time: 2021-12-04 06:14:15, Num Partitions: 1\n",
      "CEREBRO => Time: 2021-12-04 06:14:15, Writing DataFrames\n",
      "CEREBRO => Time: 2021-12-04 06:14:15, Train Data Path: file:///var/nfs/test/intermediate_train_data\n",
      "CEREBRO => Time: 2021-12-04 06:14:15, Val Data Path: file:///var/nfs/test/intermediate_val_data\n"
     ]
    }
   ],
   "source": [
    "rel = am.fit(train_df, epochs=10)\n",
    "\n",
    "import json\n",
    "m = {}\n",
    "for model in rel.metrics:\n",
    "    m[model] = {}\n",
    "    for key in rel.metrics[model]:\n",
    "        if key != 'trial':\n",
    "            m[model][key] = rel.metrics[model][key]\n",
    "\n",
    "with open(\"criteo_nas_dev/metrics.txt\", \"w\") as file:\n",
    "    file.write(json.dumps(m))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
